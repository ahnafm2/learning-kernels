 {
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy.linalg as la \n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeError(Z, w, meany, y): \n",
    "    '''COMPUTEERROR computes classification errors for ridge regression models\n",
    "    that are trained using zero-mean label vectors\n",
    "    Inputs:\n",
    "        Z the random feature matrix\n",
    "        w the optimized classication parameter vector\n",
    "        meany the mean of ytrain used to train w\n",
    "        y the true labels for datapoints in Z'''\n",
    "    \n",
    "    pred = np.sign(Z.T@ w + meany)\n",
    "    error = np.average(pred != y)\n",
    "    fn = np.average((pred != y) & (y == 1))/error\n",
    "    fp = np.average((pred != y) & (y == -1))/error\n",
    "    return error, fp, fn \n",
    "\n",
    "\n",
    "def createOptimizedGaussianKernelParams(D, W_opt, b_opt, alpha_distrib): \n",
    "    '''CREATEOPTIMIZEDGAUSSIANKERNELPARAMS generates parameters for an optimized\n",
    "    Gaussian kernel. If we ask to create more random features than the number\n",
    "    of features in the optmized kernel, we will just return the number of\n",
    "    features in the optimized kernel. If we ask for fewer, then we sample\n",
    "    from the distribution for the optimized kernel.\n",
    "    \n",
    "    Inputs:\n",
    "        D is the number of random features we wish to generate\n",
    "        W_opt, b_opt, alpha_distrib are outputs from OptimizeGaussianKernel\n",
    "    \n",
    "    Outputs:\n",
    "        D_new the number of random features we actually have now\n",
    "        W_new, b_new the parameters for those random features'''\n",
    "    \n",
    "    W_new = W_opt \n",
    "    b_new = b_opt \n",
    "    D_new = b_opt.shape[1] \n",
    "    if D < D_new: \n",
    "        p = np.random.uniform(size = (D,1))\n",
    "        summand = np.tile(p, (1, len(alpha_distrib))) - np.tile(alpha_distrib.T, (D, 1))\n",
    "        inds = np.sum(summand, axis = 1)+1\n",
    "        W_new = W_opt[:, inds]\n",
    "        b_new = b_opt[inds]\n",
    "        D_new = D \n",
    "    return D_new, W_new, b_new\n",
    "\n",
    "def createOptimizedLinearKernelParams(D, idx_opt, alpha_distrib): \n",
    "    '''CREATEOPTIMIZEDGAUSSIANKERNELPARAMS generates parameters for an optimized\n",
    "    Gaussian kernel. If we ask to create more random features than the number\n",
    "    of features in the optmized kernel, we will just return the number of\n",
    "    features in the optimized kernel. If we ask for fewer, then we sample\n",
    "    from the distribution for the optimized kernel.\n",
    "    \n",
    "    Inputs:\n",
    "        D is the number of random features we wish to generate\n",
    "        W_opt, b_opt, alpha_distrib are outputs from OptimizeGaussianKernel\n",
    "    \n",
    "    Outputs:\n",
    "        D_new the number of random features we actually have now\n",
    "        W_new, b_new the parameters for those random features'''\n",
    "    \n",
    "    idx_new = idx_opt\n",
    "    D_new = size(idx_opt, 2)\n",
    "    if D < D_new: \n",
    "        p = np.random.uniform(size = (D,1))\n",
    "        summand = np.tile(p, (1, len(alpha_distrib))) - np.tile(alpha_distrib.T, (D, 1))\n",
    "        inds = np.sum(summand, axis = 1)+1\n",
    "        idx_new = idx_opt[:, inds]\n",
    "        D_new = D \n",
    "    return D_new, idx_new\n",
    "\n",
    "def linear_chi_square(v, u, rho, acc = 1e-8):\n",
    "    '''LINEAR_CHI_SQUARE Solves the particular quadratically constrained\n",
    "    linear problem.\n",
    "\n",
    "    x = linear_chi_square(v, u, rho, acc) sets x to be the solution to the\n",
    "      optimization problem\n",
    "\n",
    "    min. x.T @ v = x.T @ Phi@y \\odot Phi@y\n",
    "    s.t. norm(x - u, 2)^2 <= rho\n",
    "         sum(x) == 1, x >= 0.\n",
    "\n",
    "    Uses a binary search strategy along with projections onto the simplex to\n",
    "    solve the problem in O(n log (n / acc)) time to solve to accuracy acc (in\n",
    "    duality gap)\n",
    "\n",
    "\n",
    "    A partial dual to the problem is given by the Lagrangian\n",
    "\n",
    "    L(x, lambda) = (lambda/2) * (norm(x - u, 2)^2 - rho) + x' * v\n",
    "    subject to     sum(x) == 1, x >= 0.\n",
    "\n",
    "    Then we maximize lambda over inf_x L(x, lambda), the infimum taken over\n",
    "    the constrained set.'''\n",
    "    \n",
    "    \n",
    "    duality_gap = np.inf\n",
    "    \n",
    "    max_lambda = np.inf\n",
    "    min_lambda = 0\n",
    "    \n",
    "    x = project_onto_simplex(u, 1) \n",
    "    \n",
    "    if (la.norm(x - u)**2 > rho): \n",
    "        raise ValueError('Problem is not feasible')\n",
    "\n",
    "    start_lambda = 1\n",
    "    while (np.isinf(max_lambda)):\n",
    "        x = project_onto_simplex(u - v/start_lambda, 1)\n",
    "        lam_grad = 0.5 * la.norm(x - u, 2)**2 - rho/2\n",
    "        if (lam_grad < 0): \n",
    "            max_lambda = start_lambda \n",
    "        else: \n",
    "            start_lambda = start_lambda * 2 \n",
    "            \n",
    "    while (max_lambda - min_lambda > acc * start_lambda): \n",
    "        lamda = (min_lambda + max_lambda) / 2 \n",
    "        x = project_onto_simplex(u - v/lamda, 1)\n",
    "        lam_grad = 0.5 * la.norm(x - u, 2)**2  - rho/2\n",
    "        if (lam_grad < 0): \n",
    "            #Then lambda is too large, so decrease max_lambda \n",
    "            max_lambda = lamda\n",
    "        else: \n",
    "            min_lambda = lamda     \n",
    "    return x\n",
    "\n",
    "def createRandomFourierFeatures(D, W, b, X): \n",
    "    '''CREATERANDOMFOURIERFEATURES creates Gaussian random features\n",
    "    Inputs:\n",
    "        D the number of features to make\n",
    "        W, b the parameters for those features (d x D and 1 x D)\n",
    "        X the datapoints to use to generate those features (d x N)'''\n",
    "    Z = np.sqrt(2/D)*np.cos(W.T @ X + b.T)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def project_onto_simplex(v, B): \n",
    "    m = len(v) \n",
    "    bget = False \n",
    "    s = np.sort(v)[::-1]\n",
    "    tmpsum = 0 \n",
    "    for ii in range(0, m-1): \n",
    "        tmpsum += s[ii]\n",
    "        tmax = (tmpsum - 1)/(ii+1)\n",
    "        if tmax >=s[ii+1]:\n",
    "            bget = True\n",
    "            break \n",
    "    if not(bget): \n",
    "        tmax = (tmpsum + s[m-1] - 1)/m\n",
    "    x = np.maximum(v - tmax, 0)\n",
    "    if sum(x) != B:\n",
    "        if np.abs(sum(x) - B) >= 1: \n",
    "            print(sum(x))\n",
    "            print(v)\n",
    "            raise ValueError('Sum way too far from B')\n",
    "    return x \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeGaussianKernel(Xtrain, ytrain, Nw, rho, tol): \n",
    "    '''OPTIMIZEGAUSSIANKERNEL optimizes random features generated for the\n",
    "    Gaussian kernel using the chi-square divergence measure.\n",
    "    See http://amansinha.org/docs/SinhaDu16.pdf for more info on the theory.\n",
    "    Inputs:\n",
    "        Xtrain is the d x N training data matrix, where N is the number of datapoints and d is the dimension.\n",
    "        ytrain is the N x 1 training label vector. The binary classes should be 1 and -1.\n",
    "        Nw is the number of random features to use.\n",
    "        rho governs the maximum allowable divergence form the original kernel distribution\n",
    "        tol is the tolerance for the solver.\n",
    "\n",
    "    Outputs: \n",
    "        W_opt is the optimized matrix of random features\n",
    "        b_opt is the optimized vector of offsets\n",
    "        alpha is the probability distribution for the random features with close-to-zero-probability features removed\n",
    "        alpha_distrib is cumulative distribution function over all random features'''\n",
    "    \n",
    "    d, _ = Xtrain.shape\n",
    "    \n",
    "    W = np.random.normal(0, 1, size = (d, Nw))\n",
    "    b = np.random.uniform(size = (1,Nw))*2*np.pi\n",
    "\n",
    "    Phi = np.cos(Xtrain.T @ W + b)\n",
    "    Ks = Phi.T @ ytrain\n",
    "    Ks = Ks**2\n",
    "    alpha_temp = linear_chi_square(-Ks, 1/Nw*np.ones((Nw,)), rho/Nw, tol) \n",
    "    \n",
    "    \n",
    "    idx = (alpha_temp > eps).reshape(-1)\n",
    "    alpha = alpha_temp[idx] \n",
    "    W_opt = W[:, idx]\n",
    "    b_opt = b[:, idx]\n",
    "    alpha_distrib = np.cumsum(alpha/sum(alpha))\n",
    "    return W_opt, b_opt, alpha, alpha_distrib\n",
    "\n",
    "def optimizeLinearKernel(Xtrain, ytrain, rho, tol): \n",
    "    '''OPTIMIZELINEARKERNEL optimizes random features generated for the\n",
    "    linear kernel using the chi-square divergence measure.\n",
    "    See http://amansinha.org/docs/SinhaDu16.pdf for more info on the theory.\n",
    "    Inputs:\n",
    "        Xtrain is the d x N training data matrix, where N is the number of datapoints and d is the dimension.\n",
    "        ytrain is the N x 1 training label vector. The binary classes should be 1 and -1.\n",
    "        rho governs the maximum allowable divergence form the original kernel distribution\n",
    "        tol is the tolerance for the solver.\n",
    "\n",
    "    Outputs: \n",
    "        idx_opt is the optimized matrix of random features\n",
    "        alpha is the probability distribution for the random features with close-to-zero-probability features removed \n",
    "        alpha_distrib is cumulative distribution function over all random features'''\n",
    "    \n",
    "    d, _ = Xtrain.shape \n",
    "    wd = np.arange(1, d+1)\n",
    "    Nw = d\n",
    "    \n",
    "    Ks = Xtrain @ ytrain \n",
    "    Ks = Ks**2 \n",
    "    \n",
    "    alpha_temp = linear_chi_square(-Ks, 1/Nw*np.ones((Nw,)), rho/Nw, tol)\n",
    "    idx = alpha_temp > eps\n",
    "    alpha = alpha_temp[idx]\n",
    "    idx_opt = wd[idx]\n",
    "    \n",
    "    alpha_distrib = np.cumsum(alpha/sum(alpha))\n",
    "    return idx_opt, alpha, alpha_distrib\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples \n",
    "Learning kernels with Random Features, example script\n",
    "\n",
    "- Generate some data\n",
    "- Create normally distributed points \n",
    "- Let the true classifier between classes be a specified radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(float).eps\n",
    "n = int(1e4)\n",
    "d = 2\n",
    "Xtrain = np.random.normal(size = (d, n))\n",
    "Xtest = np.random.normal(size = (d, int(n/10)))\n",
    "\n",
    "ytrain = np.sqrt(np.sum(Xtrain*Xtrain, axis = 0)) > np.sqrt(d)\n",
    "ytrain = ytrain*2 - 1\n",
    "ytest = np.sqrt(np.sum(Xtest*Xtest, axis = 0)) > np.sqrt(d)\n",
    "ytest = ytest*2 -1 \n",
    "\n",
    "## Optimize Kernel \n",
    "# this example uses the Gaussian Kernel and chi-square divergence \n",
    "Nw = int(2e4)\n",
    "rho = Nw*0.01\n",
    "tol = 1e-11\n",
    "\n",
    "Wopt, bopt, alpha, alpha_distrib = optimizeGaussianKernel(Xtrain, ytrain, Nw, rho, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAFACAYAAABUXgHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXlcldXa/q/FJDIroDKo4IiCCIgzKjhlzlNpWakd0zxNp7KTnfOW1nvq9P7ylNlwOnUyLU3NzDErM+dZnBABBRSZVOZhM2/2+v1xsdkMG0REQVjfz2d/2Hs/61nrXs9GvJ57X+teQkoJhUKhUCgUCoWiJWPS2AEoFAqFQqFQKBSNjRLFCoVCoVAoFIoWjxLFCoVCoVAoFIoWjxLFCoVCoVAoFIoWjxLFCoVCoVAoFIoWjxLFCoVCoVAoFIoWjxLFCoVCoVAoFIoWjxLFCoVCoVAoFIoWjxLFCoVCoVAoFIoWj1ljB6BQKBSKpo2Tk5P08PBo7DAUCoWiXpw5cyZNSul8u3aNIorVH1iFQvGgUtc/rs0JDw8PhIaGNnYYCoVCUS+EENfr0q5RRLH6A6tQKB5U6vrHVaFQKBQPFspTrFAoFAqFQqFo8ShRrFAoFAqFQqFo8aiFdgqFQqG4Y0pKSpCYmIjCwsLGDkVRBUtLS7i7u8Pc3LyxQ1EoHiiUKFYojKD+w1coYVE7iYmJsLW1hYeHB4QQjR2OogwpJdLT05GYmAhPT8/GDkeheKBQolihMIL6D79lo4TF7SksLFT/PpogQgg4OjoiNTW1sUNRKB44lKdYoTBCYWEhHB0d1X/4LRS9sFDfFNSO+vfRNFGfi0JRP5QoVihqQP3H0rJRn79CoVC0LJQoVigA6HR8NGcOHDiAY8eO3fF5oaGhePHFF2/bbsiQIfUJ67YEBwfftq75ypUrkZ+ff0/GVygUCkXLQIliRYsnJgZYs4aPyMjGjubeUZso1mq1NZ4XGBiIVatW3bb/+gjuhkKJYoVCoWjefHc8DlE3c+7pGEoUK1o0UgKHDgHTpgEzZwLHjwMlJfXrS6sFcnIaJuMcFxcHLy8vzJ07F76+vpg5c2a56Pvjjz/g7++PPn364Omnn0ZRUREAYOnSpejduzd8fX2xZMmSav198cUX+Oijj+Dn54fDhw9j3rx5eOWVVxASEoLXX38dp06dwpAhQ+Dv748hQ4bg8uXLACimJ06cCABYvnw5nn76aQQHB6NLly6VxLKNjU15++DgYMycORNeXl6YM2cOpJQAgN27d8PLywtBQUF48cUXy/utSEFBAWbPng1fX1/MmjULBQUF5ccWL16MwMBAeHt7Y9myZQCAVatWITk5GSEhIQgJCamxnULRXIiIiMCaNWuQkJCA3Nzcxg5HobjnZOUX483tl3D4Sto9HUeJYkWLpkyrwdSUj/qSnQ1s2gTs2gVs2QI0xPqsy5cvY+HChQgLC4OdnR0+//xzFBYWYt68edi0aRMuXrwIrVaLf//738jIyMDWrVtx6dIlhIWF4X/+538q9eXh4YFnn30WL7/8Ms6fP49hw4YBAK5cuYK9e/fiX//6F7y8vHDo0CGcO3cO77zzDv72t78ZjSsqKgq//fYbTp06hbfffhslRu4izp07h5UrVyIiIgJXr17F0aNHUVhYiEWLFuGXX37BkSNHalwd/+9//xtWVlYICwvD3//+d5w5c6b82LvvvovQ0FCEhYXh4MGDCAsLw4svvghXV1fs378f+/fvr7GdonmSmJiIKVOmoHv37ujatSteeuklFBcX13pOVlYWPv/880rv3an9pyHsQsuXL8eKFSvu+LySkhJ88skn2Lp1a/nNqELRnIlNzQMAdHG2vqfjKFGsaNGYmACDBwM//khRGxgI1Kcs7cWLgJcX8PjjgLMzUJZkvSs6duyIoUOHAgCeeOIJHDlyBJcvX4anpyd69OgBAJg7dy4OHToEOzs7WFpaYsGCBfjpp59gZWVVpzEeeeQRmJbdDWRnZ+ORRx6Bj48PXn75ZVy6dMnoORMmTECrVq3g5OSEdu3a4datW9XaDBgwAO7u7jAxMYGfnx/i4uIQFRWFLl26lJc4e+yxx4z2f+jQITzxxBMAAF9fX/j6+pYf++GHHxAQEAB/f39cunQJERERRvuoazvFg42UEtOnT8fUqVMRHR2NK1euQKPR4O9//3ut5xkTxXdq/2lMu1BCQgLmz5+Pbt26qUyxokUQm6oBAHR1vrc3gQ0mioUQpkKIc0KIXQ3Vp0JxP+jVC3jqKeDJJ4EK+uuOMDMD8vKA4mKgoICv75aq1Q+EEOU2hOrjm+HUqVOYMWMGtm3bhnHjxtVpDGtrw133m2++iZCQEISHh2Pnzp01liNr1apV+XNTU1OjfmRjbWqK3RjGKj9cu3YNK1aswB9//IGwsDBMmDDBaIx1bdeUEEKME0JcFkLECCGWGjneSgixqez4SSGER9n7jkKI/UIIjRDi0wrtbYUQ5ys80oQQK8uOzRNCpFY4tuB+zbOh2bdvHywtLTF//nwA/F376KOPsHr1akRERNRoQVq6dCliY2Ph5+eH1157DQDtP3rb0oIFC+Dj44M5c+Zg7969GDp0KLp3745Tp06Vj63P0H7xxRfw8/ODn58fPD09yy0869atw4ABA+Dn54dFixahtLQUAL/F6NmzJ0aPHl1uUTJGdHQ0goODERgYiL/+9a/o1q1b+bGJEydi5syZGD9+POzs7BrwiioUTZOrqXkwNxVwb9P6no7TkJt3vAQgEoD6F6p44LhbEevnB/z+O/Ddd0CnTswa3y3x8fE4fvw4Bg8ejA0bNiAoKAheXl6Ii4tDTEwMunXrhu+++w4jRoyARqNBfn4+xo8fj0GDBlX6D1SPra0tcnJqXqSQnZ0NNzc3AMCaNWvufgJV8PLywtWrVxEXFwcPDw9s2rTJaLvhw4dj/fr15QJdb33IycmBtbU17O3tcevWLfzyyy8IDg4GwLnl5ubCycmp1nZNESGEKYDPAIwBkAjgtBBih5SyYnr7TwAypZTdhBCzAfwfgFkACgG8CcCn7AEAkFLmAvCrMMYZAD9V6G+TlPL5hprD2zsvISK5YRfA9Ha1w7JJ3rW2uXTpEvr161fpPTs7O3Tq1AlarRaXL1/G119/jaFDh+Lpp5/G559/jiVLluD9999HeHg4zp8/X63PmJgYbN68GV9++SX69++P77//HkeOHMGOHTvw3nvvYdu2bZXaP/vss3j22WdRUlKCkSNH4pVXXkFkZCQ2bdqEo0ePwtzcHH/+85+xfv16eHt7Y+PGjTh37hy0Wi0CAgKqxQ8ApaWleOqpp/DZZ58hICAAL7zwAry9K1+LDh061PVSKhQPPLGpGng4WsPM9N4aHBpEFAsh3AFMAPAugFcaok+FAqDn98gRIC4OcHICRo4EKiQhmwyWlsCkSQ3bZ69evbB27VosWrQI3bt3x+LFi2FpaYlvvvkGjzzyCLRaLfr3749nn30WGRkZmDJlCgoLCyGlxEcffVStv0mTJmHmzJnYvn07Pvnkk2rH//rXv2Lu3Ln48MMPMXLkyIadDIDWrVvj888/x7hx4+Dk5IQBAwYYbbd48WLMnz8fvr6+8PPzK2/Xt29f+Pv7w9vbG126dCm3lgDAwoUL8fDDD8PFxQX79++vsV0TZQCAGCnlVQAQQmwEMAVARVE8BcDysuc/AvhUCCGklHkAjgghqt8FlSGE6A6gHYDD9yD2RkVKafRbBf37VS1Iq1atqrYItSqenp7o06cPAMDb2xujRo2CEAJ9+vRBXFxcjee99NJLGDlyJCZNmoRPP/0UZ86cQf/+/QFw8Wi7du2QkZGBadOmldubJk+ebLSvbdu2oXfv3ggICADAvwUODg61XwyFohlzNVWDbu3uvX++oTLFKwH8FYBtTQ2EEAsBLASATp06NdCwiuZOTAyQkcHqEGfPAmfOAPeoHG6Tw8TEBF988UW190eNGoVz585Ves/FxaXSV7vG6NGjR6UFZ/rFdnoGDx6MK1eulL/+3//9XwCsE6zPtC5fvrzSOeHh4eXPNRpNtfYA8Omn5d/qIyQkBFFRUZBS4rnnnkNgYGC1OFu3bo2NGzcanUNNGewXXngBL7zwwm3bNVHcACRUeJ0IYGBNbaSUWiFENgBHAHVZiv0YmBmu6F+ZIYQYDuAKgJellAnGT60bt8vo3iu8vb2xZcuWSu/l5OQgISEBpqamRi1It6Oi9cfExKT8tYmJSY2lC9esWYPr16+X/65LKTF37lz885//rNRu5cqVdYrh3Llz8PMrT/TjwoULGDNmzG3PUyiaIyWlOlxPz8dD3vf+25G7zkMLISYCSJFSnqmtnZTySylloJQy0NnZ+W6HVbQQiooAW1vAxgZo04avFQ8uX331Ffz8/ODt7Y3s7GwsWrSosUNqChhTSVUN2HVpUxOzAWyo8HonAA8ppS+AvQDWGg1KiIVCiFAhRGhNlUIam1GjRiE/Px/ffvstANoOXn31VcybNw9WVlblFiQA5RYkwGC3aQjOnDmDFStWYN26dTAxMSmP68cff0RKSgoAICMjA9evX8fw4cOxdetWFBQUIDc3Fzt37jTap6OjI6KiogAAJ0+exLfffltpwalC0ZKIz8iHVifv+SI7oGEW2g0FMFkIEQdgI4CRQoh1DdCvQoFu3YDUVGDzZuD8eaDsW81mj4eHR6UsbHNBXxIuIiIC69evr3OVjGZOIoCOFV67A0iuqY0QwgyAPYCM23UshOgLwKxi0kJKmS6l1N9efgWguqkVD0YiQwiBrVu3YvPmzejevTt69OgBS0tLvPfeewAMFiRfX19kZGRg8eLFACg6hw4dCh8fn/KFdvXl008/RUZGBkJCQuDn54cFCxagd+/e+Mc//oGxY8fC19cXY8aMwY0bNxAQEIBZs2bBz88PM2bMqPZtjZ4nn3wSoaGh6NOnD3766Sc4OjoaXSegULQErt6ncmwAIO5kRfhtOxMiGMASKWX1ivwVCAwMlLfbtlWh0KPVAllZzBjfLz9xZGQkevXqdX8GUzRZjP0eCCHOSCmr+z7qSZnIvQJgFIAkAKcBPC6lvFShzXMA+kgpny1baDddSvlohePzAARWXTwnhHgfQJGUclmF91yklDfKnk8D8LqUclBtMRr7m93U/43ExcVh4sSJD/zNZUJCAmbOnImTJ0/e0XlN/fNRKOrKFwdj8f4vUbiwbCzsW9ejZirq/ne7IatPKBT3BDMzLrJTKJojZR7h5wH8BsAUwGop5SUhxDsAQqWUOwB8DeA7IUQMmCGerT+/7Fs6OwAWQoipAMZWqFzxKIDxVYZ8UQgxGYC2rK9592xyirvmwoULyjqhaNFcTdXAyaZVvQXxndCgolhKeQDAgYbsU6FQKJo7UsrdAHZXee+tCs8LATxSw7ketfTbxch7bwB4o76xPig0FwvSxIkTjW6HrlC0FGJT89D1PlgnALWjnUKhUCgUCoWiCXLoSirCk7LRo32Nxc0aFCWKFYoWhLHtbevK+PHjkZWVVWubt956C3v37q1X/7WxZs0aPP987XtNHDhwoFG33lUoFApFw5BdUILP9sdg/prT8HSyxp9Dut6XcZWnWKFoQehF8Z///Odqx0pLS2Fqalrjubt3767xmJ533nnnruK7Gw4cOAAbGxsMaSmFrBUKhaIZ8t2J63j35wgUlugwtnd7fDjLDzat7o9cVZlihaKeFBYCmZlAdjZQWgqWycjJAXS6u+47Li4OvXr1wjPPPANvb2+MHTsWBQUFAIDY2FiMGzcO/fr1w7Bhw8rrmcbGxmLQoEHo378/3nrrLdjYVK/puHTpUsTGxsLPzw+vvfYaDhw4gJCQEDz++OPlu3hNnToV/fr1g7e3N7788svycz08PJCWllZrbPPmzcOPP/5Y3n7ZsmUICAhAnz59yuNMTU3FmDFjEBAQgEWLFqFz585IS6u+B8U333yDHj16YMSIETh69Gj5+zt37sTAgQPh7++P0aNH49atW4iLi8MXX3yBjz76CH5+fjh8+LDRdoqGpSGrFykaDvW5KB5UdDqJj/dGw6uDHXa9EIQvnwq8b4IYUKJYoagXWi2QlwdYW7M6hiYpG9i0Cdi1C9iyhYr5LomOjsZzzz2HS5cuwcHBoXznroULF+KTTz4p3zRAn/V96aWX8NJLL+H06dNwdXU12uf777+Prl274vz58/jggw8AAKdOncK7776LiAgWLFi9ejXOnDmD0NBQrFq1Cunp6XWOrSpOTk44e/YsFi9ejBUrVgAA3n77bYwcORJnz57FtGnTEB8fX+28GzduYNmyZTh69Ch+//338tgAICgoCCdOnMC5c+cwe/Zs/L//9//g4eGBZ599trwO8rBhw4y2UzQclpaWSE9PVwKsiSGlRHp6OiwtLRs7FIXijglPzkaapghPDuoMHzf7+z6+sk8omhQZGaxJ7OoKNOW/6TodYGICWFgAQgCl4RcBLy+gXz/gwAHg8mWgb9+7GsPT07N8q9d+/fohLi4OGo0Gx44dwyOPGAoRFJVt83f8+HFs27YNAPD4449jyZIldRpnwIAB8PT0LH+9atUqbN26FQBrpEZHR8PR0fG2sRlj+vTp5W1++uknAMCRI0fK+x83bhzatGlT7byTJ08iODgY+k0jZs2aVb4FdWJiImbNmoUbN26guLi4UuwVqWs7Rf1wd3dHYmIimupudy0ZS0tLuLu7N3YYCsUdsy8qBUIAwT0bZ8MgJYoVTYZr14AjRwBnZ+DECWDaNKB168aOyjjmZeUSMzMpkK2tzZg6Li4GCgo4ibukVYWdSkxNTVFQUACdTgcHBwecP3/+rvvXY21tKHVz4MAB7N27F8ePH4eVlRWCg4NRaCTrbSy22uZgamoKrVYLoO5f7QphbGdj4IUXXsArr7yCyZMn48CBA1i+fPldtVPUD3Nzc3WjoVAoGpT9USnw6+gAR5v7tFNXFZR9QtFkiIoCgoKAceOAdu0AI9+qNxmEABwcaJ+wtwcsB/rRXPzdd/RTeHndk3Ht7Ozg6emJzZs3A6DAvHDhAgBg0KBB5TaGjRs3Gj3f1tYWubm5NfafnZ2NNm3awMrKClFRUThx4kQDz4D2hx9++AEAsGfPHmRmZlZrM3DgQBw4cADp6ekoKSkpn68+Rjc3NwDA2rVry9+vOrea2ikUCoWi6ZGaW4QLidkY2bNdo8WgRLGiyWBry2xxUhKQksI1a7Gx9O82RaQEioqYIC4SlsCkScCf/gSMGQPUUsXhblm/fj2+/vpr9O3bF97e3ti+fTsAYOXKlfjwww8xYMAA3LhxA/b21f1Yjo6OGDp0KHx8fPDaa69VOz5u3DhotVr4+vrizTffxKBBte7+Wy+WLVuGPXv2ICAgAL/88gtcXFxga1u5BqWLiwuWL1+OwYMHY/To0QgICCg/tnz5cjzyyCMYNmwYnCpsdThp0iRs3bq1fKFdTe0UCoVC0fQ4cDkFABDi1XiiWDTGIonAwEAZGhp638dVNG1KSoDjx2lJyMlhFtbCgvaESZOYnb1fREZGolevXrW2yclhTBYWQG4uM8dmjWhIys/PR+vWrSGEwMaNG7Fhw4ZywdyUKCoqgqmpKczMzHD8+HEsXry4Qe0gDYmx3wMhxBkpZWAjhdQoqL/ZCoXiXpKZV4wZ/z6GIq0OR14PqdE+V1/q+ndbeYoVTQZzc2D4cD5fvRqYMIGCc/16ik47u8aNryqlpYCNDeMuKODrxhTFZ86cwfPPPw8pJRwcHLB69erGC6YW4uPj8eijj0Kn08HCwgJfffVVY4ekUCgUikaisKQUC78LRWJWAdYvGNjggvhOUKJY0SSxtwfCwyk6dbqmueCuVSuKdTMzCmL94rvGYtiwYeX+4qZM9+7dce7cucYOQ6FQKBRNgNVHr+F0XCY+ecwf/T3aNmosylOsaJKMHUsbxbVrXHjX2ILTGFZWFO0WFkCbNizRplAoFAqFou6cj89CV2drTOprvL7+/URlihVNEltbYPTox
